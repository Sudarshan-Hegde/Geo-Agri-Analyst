{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a53730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m481.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhegdesudarshan\u001b[0m (\u001b[33mhegdesudarshan-hegde\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì WandB authenticated\n",
      "\n",
      "üöÄ Device: cuda | GPUs: 2\n",
      "   GPU 0: Tesla T4\n",
      "   GPU 1: Tesla T4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251125_114305-g6x3mxih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hegdesudarshan-hegde/SR-ResNet-AL-Classification/runs/g6x3mxih' target=\"_blank\">SR-ResNet-AL-20251125-114305</a></strong> to <a href='https://wandb.ai/hegdesudarshan-hegde/SR-ResNet-AL-Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hegdesudarshan-hegde/SR-ResNet-AL-Classification' target=\"_blank\">https://wandb.ai/hegdesudarshan-hegde/SR-ResNet-AL-Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hegdesudarshan-hegde/SR-ResNet-AL-Classification/runs/g6x3mxih' target=\"_blank\">https://wandb.ai/hegdesudarshan-hegde/SR-ResNet-AL-Classification/runs/g6x3mxih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Setup complete!\n",
      "Config: {'dataset_size': 50000, 'num_classes': 19, 'sr_model_path': '/kaggle/input/sr-model/pytorch/default/3/generator_ensemble.pth', 'lr_size': 32, 'hr_size': 128, 'clf_epochs': 20, 'batch_size': 32, 'lr': 0.0001, 'weight_decay': 0.0001, 'al_cycles': 4, 'al_epochs_per_cycle': 10, 'initial_labeled_ratio': 0.1, 'query_size_ratio': 0.1, 'num_workers': 4, 'pin_memory': True, 'mixed_precision': True}\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 1: Dependencies & WandB Setup\n",
    "# ===============================================================================\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q wandb scikit-learn matplotlib seaborn tqdm Pillow rasterio pandas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import rasterio\n",
    "import wandb\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# WandB Login\n",
    "wandb.login(key=\"5424a3d65aac1662f5be82d4439aaac35046689e\")\n",
    "print(\"‚úì WandB authenticated\")\n",
    "\n",
    "# Device Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"\\nüöÄ Device: {device} | GPUs: {gpu_count}\")\n",
    "if gpu_count > 0:\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Optimized Config for 12h training with enhanced accuracy\n",
    "config = {\n",
    "    # Dataset\n",
    "    'dataset_size': 60000,  # Increased from 50k (still under 12h)\n",
    "    'num_classes': 19,      # BigEarthNet-19 classes\n",
    "    \n",
    "    # SR Model\n",
    "    'sr_model_path': '/kaggle/input/sr-model/pytorch/default/3/generator_ensemble.pth',\n",
    "    'lr_size': 32,          # LR input size\n",
    "    'hr_size': 128,         # HR output size (32*4)\n",
    "    \n",
    "    # Classifier Training - OPTIMIZED\n",
    "    'clf_epochs': 30,       # Increased from 20 for better convergence\n",
    "    'batch_size': 48,       # Increased from 32 (faster training)\n",
    "    'lr': 2e-4,             # Increased from 1e-4 (faster learning)\n",
    "    'weight_decay': 5e-5,   # Reduced from 1e-4 (less regularization)\n",
    "    'warmup_epochs': 3,     # NEW: Warmup for stable start\n",
    "    'label_smoothing': 0.1, # NEW: Better generalization\n",
    "    \n",
    "    # Active Learning\n",
    "    'al_cycles': 4,\n",
    "    'al_epochs_per_cycle': 10,\n",
    "    'initial_labeled_ratio': 0.1,\n",
    "    'query_size_ratio': 0.1,\n",
    "    \n",
    "    # Training Enhancements\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    'mixed_precision': True,\n",
    "    'gradient_clip': 1.0,   # NEW: Prevent gradient explosion\n",
    "    'ema_decay': 0.999,     # NEW: Exponential Moving Average for stability\n",
    "}\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"SR-ResNet-AL-Classification\",\n",
    "    config=config,\n",
    "    name=f\"SR-ResNet-AL-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Setup complete!\")\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ebcd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained SR model...\n",
      "‚úì SR Model loaded: torch.Size([1, 3, 32, 32]) ‚Üí torch.Size([1, 3, 256, 256])\n",
      "  Parameters: 9.77M\n",
      "\n",
      "‚úì SR model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 2: Load Pre-trained SR Model (EXACT ARCHITECTURE FROM CHECKPOINT)\n",
    "# ===============================================================================\n",
    "\n",
    "class RFB(nn.Module):\n",
    "    \"\"\"Receptive Field Block - EXACT match to checkpoint\"\"\"\n",
    "    def __init__(self, in_channels=64):\n",
    "        super().__init__()\n",
    "        # Branch 1: AvgPool(3) + Conv + ReLU + Conv\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 16, 1, 1, 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, 3, 1, padding=1, dilation=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Branch 2: AvgPool(5) + Conv + ReLU + Conv\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.AvgPool2d(5, stride=1, padding=2),\n",
    "            nn.Conv2d(in_channels, 24, 1, 1, 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(24, 24, 3, 1, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Branch 3: AvgPool(7) + Conv + ReLU + Conv\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(7, stride=1, padding=3),\n",
    "            nn.Conv2d(in_channels, 24, 1, 1, 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(24, 24, 3, 1, padding=3, dilation=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Changed to match checkpoint: conv_concat instead of conv\n",
    "        self.conv_concat = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 1, 1, 0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        out = torch.cat([b1, b2, b3], 1)\n",
    "        return self.conv_concat(out) * 0.2 + x\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"Dense Block with 5 conv layers - MODIFIED channel counts to match checkpoint\"\"\"\n",
    "    def __init__(self, nf=64):\n",
    "        super().__init__()\n",
    "        # Changed: nf=64 in Generator, but DenseBlock uses nf_internal=32\n",
    "        nf_internal = 32\n",
    "        self.conv1 = nn.Conv2d(nf, nf_internal, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nf + nf_internal, nf_internal, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nf + nf_internal*2, nf_internal, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nf + nf_internal*3, nf_internal, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nf + nf_internal*4, nf, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(torch.cat([x, x1], 1)))\n",
    "        x3 = F.relu(self.conv3(torch.cat([x, x1, x2], 1)))\n",
    "        x4 = F.relu(self.conv4(torch.cat([x, x1, x2, x3], 1)))\n",
    "        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    \"\"\"Residual-in-Residual Dense Block (3 DenseBlocks)\"\"\"\n",
    "    def __init__(self, nf=64):\n",
    "        super().__init__()\n",
    "        self.db1 = DenseBlock(nf)\n",
    "        self.db2 = DenseBlock(nf)\n",
    "        self.db3 = DenseBlock(nf)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.db3(self.db2(self.db1(x)))\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class RRFDB(nn.Module):\n",
    "    \"\"\"Residual RFB Dense Block - MODIFIED to match checkpoint structure\"\"\"\n",
    "    def __init__(self, nf=64):\n",
    "        super().__init__()\n",
    "        # Changed: Use named attributes instead of ModuleList to match checkpoint keys\n",
    "        self.rfb1 = RFB(nf)\n",
    "        self.rfb2 = RFB(nf)\n",
    "        self.rfb3 = RFB(nf)\n",
    "        self.rfb4 = RFB(nf)\n",
    "        self.rfb5 = RFB(nf)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.rfb1(x)\n",
    "        out = self.rfb2(out)\n",
    "        out = self.rfb3(out)\n",
    "        out = self.rfb4(out)\n",
    "        out = self.rfb5(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator: 12 RRDB + 6 RRFDB + 8x upscale - EXACT architecture from checkpoint\"\"\"\n",
    "    def __init__(self, num_rrdb=12, num_rrfdb=6, nf=64):\n",
    "        super().__init__()\n",
    "        self.conv_first = nn.Conv2d(3, nf, 3, 1, 1)\n",
    "        \n",
    "        # Trunk A: 12 RRDB blocks\n",
    "        self.trunk_a = nn.Sequential(*[RRDB(nf) for _ in range(num_rrdb)])\n",
    "        \n",
    "        # Trunk RFB: 6 RRFDB blocks\n",
    "        self.trunk_rfb = nn.Sequential(*[RRFDB(nf) for _ in range(num_rrfdb)])\n",
    "        \n",
    "        # RFB upsampling\n",
    "        self.rfb_up = RFB(nf)\n",
    "        \n",
    "        # 8x upscaling (3 PixelShuffle layers: 2x each = 2^3 = 8x)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf*4, 3, 1, 1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nf, nf*4, 3, 1, 1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(nf, nf*4, 3, 1, 1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Changed to match checkpoint: Sequential with conv_final layers\n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nf, 3, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv_first(x)\n",
    "        trunk_a_out = self.trunk_a(feat)\n",
    "        trunk_rfb_out = self.trunk_rfb(trunk_a_out)\n",
    "        rfb_up_out = self.rfb_up(trunk_rfb_out)\n",
    "        up = self.upsample(rfb_up_out + feat)\n",
    "        return torch.tanh(self.conv_final(up))\n",
    "\n",
    "# Load Pre-trained SR Model\n",
    "print(\"Loading pre-trained SR model...\")\n",
    "sr_model = Generator(num_rrdb=12, num_rrfdb=6, nf=64).to(device)\n",
    "\n",
    "try:\n",
    "    state_dict = torch.load(config['sr_model_path'], map_location=device)\n",
    "    sr_model.load_state_dict(state_dict)\n",
    "    sr_model.eval()\n",
    "    \n",
    "    # Test SR model\n",
    "    with torch.no_grad():\n",
    "        test_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "        test_output = sr_model(test_input)\n",
    "        print(f\"‚úì SR Model loaded: {test_input.shape} ‚Üí {test_output.shape}\")\n",
    "        \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in sr_model.parameters())\n",
    "    print(f\"  Parameters: {total_params/1e6:.2f}M\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading SR model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Freeze SR model (no training needed)\n",
    "for param in sr_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"\\n‚úì SR model ready for inference!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f95c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BigEarthNet dataset...\n",
      "Found 347244 band files\n",
      "Valid RGB patches: 28937\n",
      "Warning: metadata.parquet not found, using dummy labels\n",
      "Split: 23149 train, 5788 val\n",
      "\n",
      "‚úì Dataset loaded!\n",
      "  Train batches: 724\n",
      "  Val batches: 91\n",
      "\n",
      "Sample batch shapes:\n",
      "  LR: torch.Size([32, 3, 32, 32])\n",
      "  Label: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 3: Dataset Loading (BigEarthNet)\n",
    "# ===============================================================================\n",
    "\n",
    "class BigEarthNetDataset(Dataset):\n",
    "    \"\"\"BigEarthNet dataset with SR preprocessing\"\"\"\n",
    "    def __init__(self, root_path, patch_ids, patch_to_bands, patch_to_label, \n",
    "                 sr_model=None, phase='train'):\n",
    "        self.root_path = root_path\n",
    "        self.patch_ids = patch_ids\n",
    "        self.patch_to_bands = patch_to_bands\n",
    "        self.patch_to_label = patch_to_label\n",
    "        self.sr_model = sr_model\n",
    "        self.phase = phase\n",
    "        \n",
    "        # Transforms\n",
    "        if phase == 'train':\n",
    "            self.spatial_aug = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomRotation(90)\n",
    "            ])\n",
    "        else:\n",
    "            self.spatial_aug = None\n",
    "            \n",
    "        self.to_tensor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patch_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patch_id = self.patch_ids[idx]\n",
    "        bands = self.patch_to_bands[patch_id]\n",
    "        \n",
    "        try:\n",
    "            # Load RGB bands (B04=Red, B03=Green, B02=Blue)\n",
    "            b02 = rasterio.open(bands['02']).read(1).astype(np.float32) / 10000.0\n",
    "            b03 = rasterio.open(bands['03']).read(1).astype(np.float32) / 10000.0\n",
    "            b04 = rasterio.open(bands['04']).read(1).astype(np.float32) / 10000.0\n",
    "            \n",
    "            # Stack to RGB (120x120)\n",
    "            hr_np = np.stack([b04, b03, b02], axis=-1)\n",
    "            hr_np = np.clip(hr_np, 0, 1)\n",
    "            \n",
    "            # Convert to PIL for augmentation\n",
    "            hr_pil = Image.fromarray((hr_np * 255).astype(np.uint8))\n",
    "            \n",
    "            # Apply spatial augmentation\n",
    "            if self.spatial_aug:\n",
    "                hr_pil = self.spatial_aug(hr_pil)\n",
    "            \n",
    "            # Resize to 32x32 for LR input\n",
    "            lr_pil = hr_pil.resize((32, 32), Image.BICUBIC)\n",
    "            \n",
    "            # To tensor\n",
    "            lr_tensor = self.to_tensor(lr_pil)\n",
    "            \n",
    "            # Get label (multi-hot ‚Üí single label via argmax)\n",
    "            label_multihot = self.patch_to_label.get(patch_id, torch.zeros(config['num_classes']))\n",
    "            label = torch.argmax(label_multihot).long()\n",
    "            \n",
    "            return {\n",
    "                'lr': lr_tensor,\n",
    "                'label': label,\n",
    "                'patch_id': patch_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return black dummy on error\n",
    "            print(f\"Error loading {patch_id}: {e}\")\n",
    "            return {\n",
    "                'lr': torch.zeros(3, 32, 32),\n",
    "                'label': torch.tensor(0, dtype=torch.long),\n",
    "                'patch_id': patch_id\n",
    "            }\n",
    "\n",
    "# Load BigEarthNet metadata\n",
    "print(\"Loading BigEarthNet dataset...\")\n",
    "image_root_path = '/kaggle/input/bigearthnetv2-s2-4/'\n",
    "\n",
    "# Find all TIF files\n",
    "import glob\n",
    "all_tif_paths = glob.glob(os.path.join(image_root_path, '**/*.tif'), recursive=True)\n",
    "print(f\"Found {len(all_tif_paths)} band files\")\n",
    "\n",
    "# Group by patch ID\n",
    "patch_to_bands = {}\n",
    "for path in all_tif_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    if '_B' in fname:\n",
    "        patch_id = '_'.join(fname.split('_B')[:-1])\n",
    "        band = fname.split('_B')[-1].split('.')[0]\n",
    "        if patch_id not in patch_to_bands:\n",
    "            patch_to_bands[patch_id] = {}\n",
    "        patch_to_bands[patch_id][band] = path\n",
    "\n",
    "# Filter patches with RGB bands\n",
    "valid_patches = [pid for pid, bands in patch_to_bands.items() \n",
    "                 if all(b in bands for b in ['02', '03', '04'])]\n",
    "valid_patches = valid_patches[:config['dataset_size']]\n",
    "print(f\"Valid RGB patches: {len(valid_patches)}\")\n",
    "\n",
    "# Load labels from metadata\n",
    "metadata_path = os.path.join(image_root_path, 'metadata.parquet')\n",
    "if os.path.exists(metadata_path):\n",
    "    df = pd.read_parquet(metadata_path)\n",
    "    patch_to_label = {}\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row['patch_id']\n",
    "        labels_list = row['labels'] if isinstance(row['labels'], list) else []\n",
    "        multi_hot = torch.zeros(config['num_classes'])\n",
    "        for lbl in labels_list:\n",
    "            if 0 <= lbl < config['num_classes']:\n",
    "                multi_hot[lbl] = 1.0\n",
    "        if pid in valid_patches:\n",
    "            patch_to_label[pid] = multi_hot\n",
    "    print(f\"Loaded labels for {len(patch_to_label)} patches\")\n",
    "else:\n",
    "    print(\"Warning: metadata.parquet not found, using dummy labels\")\n",
    "    patch_to_label = {pid: torch.zeros(config['num_classes']) for pid in valid_patches}\n",
    "\n",
    "# Train/Val split\n",
    "train_ids, val_ids = train_test_split(valid_patches, test_size=0.2, random_state=42)\n",
    "print(f\"Split: {len(train_ids)} train, {len(val_ids)} val\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BigEarthNetDataset(image_root_path, train_ids, patch_to_bands, \n",
    "                                   patch_to_label, sr_model, phase='train')\n",
    "val_dataset = BigEarthNetDataset(image_root_path, val_ids, patch_to_bands, \n",
    "                                 patch_to_label, sr_model, phase='val')\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
    "                         shuffle=True, num_workers=config['num_workers'], \n",
    "                         pin_memory=config['pin_memory'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size']*2, \n",
    "                       shuffle=False, num_workers=config['num_workers'], \n",
    "                       pin_memory=config['pin_memory'])\n",
    "\n",
    "print(\"\\n‚úì Dataset loaded!\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  LR: {sample_batch['lr'].shape}\")\n",
    "print(f\"  Label: {sample_batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f25374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up GPU memory...\n",
      "‚úì GPU memory cleaned\n",
      "  GPU 0: 0.07GB allocated, 0.08GB reserved\n",
      "  GPU 1: 0.00GB allocated, 0.00GB reserved\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 3.5: GPU Memory Cleanup (Run this if you encounter CUDA errors)\n",
    "# ===============================================================================\n",
    "\n",
    "print(\"Cleaning up GPU memory...\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reset CUDA device if needed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    print(f\"‚úì GPU memory cleaned\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        mem_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        mem_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"  GPU {i}: {mem_allocated:.2f}GB allocated, {mem_reserved:.2f}GB reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SR-Enhanced ResNet50 Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 151MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Classifier moved to GPU\n",
      "  Using DataParallel across 2 GPUs\n",
      "\n",
      "‚úì Classifier created\n",
      "  Total parameters: 33.32M\n",
      "  Trainable parameters: 23.55M\n",
      "\n",
      "Test forward pass: torch.Size([2, 3, 32, 32]) ‚Üí torch.Size([2, 19])\n",
      "Output range: [-0.144, 0.135]\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 4: ResNet Classifier Definition\n",
    "# ===============================================================================\n",
    "\n",
    "class SREnhancedClassifier(nn.Module):\n",
    "    \"\"\"ResNet50-based classifier that processes SR-enhanced images\"\"\"\n",
    "    def __init__(self, num_classes, sr_model, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.sr_model = sr_model  # Frozen SR model\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        if pretrained:\n",
    "            weights = ResNet50_Weights.IMAGENET1K_V2\n",
    "            self.backbone = resnet50(weights=weights)\n",
    "        else:\n",
    "            self.backbone = resnet50(weights=None)\n",
    "        \n",
    "        # Replace final FC layer with enhanced classifier head\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.4),  # Increased dropout\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lr_images):\n",
    "        # Step 1: SR enhancement (frozen) - 32x32 ‚Üí 256x256\n",
    "        with torch.no_grad():\n",
    "            sr_images = self.sr_model(lr_images)\n",
    "            # Resize 256x256 ‚Üí 224x224 for ResNet50\n",
    "            sr_images = F.interpolate(sr_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Step 2: ResNet classification\n",
    "        x = self.backbone.conv1(sr_images)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        \n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        \n",
    "        x = self.backbone.avgpool(x)\n",
    "        features = torch.flatten(x, 1)\n",
    "        output = self.backbone.fc(features)\n",
    "        return output\n",
    "    \n",
    "    def get_features(self, lr_images):\n",
    "        \"\"\"Extract features for active learning\"\"\"\n",
    "        with torch.no_grad():\n",
    "            sr_images = self.sr_model(lr_images)\n",
    "            sr_images = F.interpolate(sr_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            x = self.backbone.conv1(sr_images)\n",
    "            x = self.backbone.bn1(x)\n",
    "            x = self.backbone.relu(x)\n",
    "            x = self.backbone.maxpool(x)\n",
    "            \n",
    "            x = self.backbone.layer1(x)\n",
    "            x = self.backbone.layer2(x)\n",
    "            x = self.backbone.layer3(x)\n",
    "            x = self.backbone.layer4(x)\n",
    "            \n",
    "            x = self.backbone.avgpool(x)\n",
    "            return torch.flatten(x, 1)\n",
    "\n",
    "# Create classifier with error handling\n",
    "print(\"Creating SR-Enhanced ResNet50 Classifier...\")\n",
    "\n",
    "try:\n",
    "    # Clean GPU memory before creating model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Create classifier on CPU first\n",
    "    classifier = SREnhancedClassifier(config['num_classes'], sr_model, pretrained=True)\n",
    "    \n",
    "    # Move to GPU carefully\n",
    "    classifier = classifier.to(device)\n",
    "    print(\"‚úì Classifier moved to GPU\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e):\n",
    "        print(f\"‚ö† CUDA Error: {e}\")\n",
    "        print(\"Attempting recovery: Restarting kernel may help\")\n",
    "        print(\"Run the memory cleanup cell (Cell 3.5) and try again\")\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "if gpu_count > 1:\n",
    "    classifier = nn.DataParallel(classifier)\n",
    "    print(f\"  Using DataParallel across {gpu_count} GPUs\")\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in classifier.parameters())\n",
    "print(f\"\\n‚úì Classifier created\")\n",
    "print(f\"  Total parameters: {total_params/1e6:.2f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "\n",
    "# Test forward pass\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        test_lr = torch.randn(2, 3, 32, 32).to(device)\n",
    "        test_output = classifier(test_lr)\n",
    "        print(f\"\\nTest forward pass: {test_lr.shape} ‚Üí {test_output.shape}\")\n",
    "        print(f\"Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ö† Test forward pass failed: {e}\")\n",
    "    print(\"This may indicate GPU memory issues. Try restarting the kernel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ba4d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training functions defined\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 5: Training & Evaluation Functions\n",
    "# ===============================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        lr_imgs = batch['lr'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if config['mixed_precision']:\n",
    "            with autocast():\n",
    "                outputs = model(lr_imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(lr_imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * lr_imgs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            lr_imgs = batch['lr'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            if config['mixed_precision']:\n",
    "                with autocast():\n",
    "                    outputs = model(lr_imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                outputs = model(lr_imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * lr_imgs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro') * 100\n",
    "    avg_loss = total_loss / len(all_labels)\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, epoch):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# CELL 5.5: Comprehensive Evaluation Metrics & Visualization Functions\n",
    "# ===============================================================================\n",
    "\n",
    "def compute_per_class_metrics(y_true, y_pred, num_classes=19):\n",
    "    \"\"\"Compute precision, recall, F1 per class\"\"\"\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=list(range(num_classes)), zero_division=0\n",
    "    )\n",
    "    return precision, recall, f1, support\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].set_title('Accuracy Curves')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 0].plot(history['val_f1'], label='Val F1 (Macro)', linewidth=2, color='green')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score (%)')\n",
    "    axes[1, 0].set_title('F1 Score Curve')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 1].plot(history['lr'], linewidth=2, color='red')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"training_curves\": wandb.Image(plt)})\n",
    "    plt.savefig('/kaggle/working/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_per_class_performance(precision, recall, f1, support, class_names=None):\n",
    "    \"\"\"Plot per-class metrics\"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i}' for i in range(len(precision))]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    x = np.arange(len(class_names))\n",
    "    \n",
    "    # Precision\n",
    "    axes[0, 0].bar(x, precision * 100, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Class')\n",
    "    axes[0, 0].set_ylabel('Precision (%)')\n",
    "    axes[0, 0].set_title('Per-Class Precision')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 0].axhline(y=precision.mean()*100, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Recall\n",
    "    axes[0, 1].bar(x, recall * 100, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Class')\n",
    "    axes[0, 1].set_ylabel('Recall (%)')\n",
    "    axes[0, 1].set_title('Per-Class Recall')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0, 1].axhline(y=recall.mean()*100, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[1, 0].bar(x, f1 * 100, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Class')\n",
    "    axes[1, 0].set_ylabel('F1 Score (%)')\n",
    "    axes[1, 0].set_title('Per-Class F1 Score')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1, 0].axhline(y=f1.mean()*100, color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Support (sample count)\n",
    "    axes[1, 1].bar(x, support, color='plum', edgecolor='purple', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Class')\n",
    "    axes[1, 1].set_ylabel('Sample Count')\n",
    "    axes[1, 1].set_title('Per-Class Sample Distribution')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(class_names, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"per_class_metrics\": wandb.Image(plt)})\n",
    "    plt.savefig('/kaggle/working/per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model weights\"\"\"\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "    \n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "    \n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "    \n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "    \n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "print(\"‚úì Comprehensive evaluation metrics & EMA defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500aee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING FULL TRAINING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfba18f924c748f5bfcc204ac1169db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/2693479398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     train_loss, train_acc = train_epoch(classifier, train_loader, criterion, \n\u001b[0m\u001b[1;32m     23\u001b[0m                                         optimizer, scaler, device)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_38/3037286883.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer, scaler, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed_precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# CELL 6: Full Training Pipeline with Active Learning\n",
    "# ===============================================================================\n",
    "\n",
    "# Setup with label smoothing and warmup\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=config['lr'], \n",
    "                        weight_decay=config['weight_decay'])\n",
    "\n",
    "# Warmup + Cosine Annealing scheduler\n",
    "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, \n",
    "                                               total_iters=config['warmup_epochs'])\n",
    "main_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                      T_max=config['clf_epochs'] - config['warmup_epochs'],\n",
    "                                                      eta_min=1e-6)\n",
    "scheduler = optim.lr_scheduler.SequentialLR(optimizer, \n",
    "                                            schedulers=[warmup_scheduler, main_scheduler],\n",
    "                                            milestones=[config['warmup_epochs']])\n",
    "\n",
    "scaler = GradScaler() if config['mixed_precision'] else None\n",
    "\n",
    "# Initialize EMA\n",
    "ema = EMA(classifier, decay=config['ema_decay'])\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING OPTIMIZED TRAINING PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìä Enhancements: Label Smoothing, Warmup LR, EMA, Enhanced Metrics\")\n",
    "print(f\"‚è±Ô∏è  Estimated Time: ~2.5-3 hours for 30 epochs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initial supervised training with enhanced evaluation\n",
    "best_val_acc = 0\n",
    "best_val_f1 = 0\n",
    "\n",
    "for epoch in range(config['clf_epochs']):\n",
    "    epoch_start = time.time()\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['clf_epochs']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(classifier, train_loader, criterion, \n",
    "                                        optimizer, scaler, device)\n",
    "    \n",
    "    # Update EMA\n",
    "    ema.update()\n",
    "    \n",
    "    # Validate with original weights\n",
    "    val_loss, val_acc, val_f1, val_preds, val_labels = evaluate(classifier, val_loader, \n",
    "                                                                 criterion, device)\n",
    "    \n",
    "    # Validate with EMA weights\n",
    "    ema.apply_shadow()\n",
    "    val_loss_ema, val_acc_ema, val_f1_ema, val_preds_ema, val_labels_ema = evaluate(\n",
    "        classifier, val_loader, criterion, device)\n",
    "    ema.restore()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Use EMA results if better\n",
    "    use_ema = val_acc_ema > val_acc\n",
    "    final_val_acc = val_acc_ema if use_ema else val_acc\n",
    "    final_val_f1 = val_f1_ema if use_ema else val_f1\n",
    "    final_val_loss = val_loss_ema if use_ema else val_loss\n",
    "    final_preds = val_preds_ema if use_ema else val_preds\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(final_val_loss)\n",
    "    history['val_acc'].append(final_val_acc)\n",
    "    history['val_f1'].append(final_val_f1)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Compute per-class metrics\n",
    "    precision, recall, f1_per_class, support = compute_per_class_metrics(\n",
    "        val_labels_ema if use_ema else val_labels, final_preds, config['num_classes']\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train/loss': train_loss,\n",
    "        'train/accuracy': train_acc,\n",
    "        'val/loss': final_val_loss,\n",
    "        'val/accuracy': final_val_acc,\n",
    "        'val/f1_macro': final_val_f1,\n",
    "        'val/precision_macro': precision.mean() * 100,\n",
    "        'val/recall_macro': recall.mean() * 100,\n",
    "        'val_ema/accuracy': val_acc_ema,\n",
    "        'val_ema/f1_macro': val_f1_ema,\n",
    "        'lr': optimizer.param_groups[0]['lr'],\n",
    "        'epoch_time': time.time() - epoch_start\n",
    "    })\n",
    "    \n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val   - Loss: {final_val_loss:.4f}, Acc: {final_val_acc:.2f}%, F1: {final_val_f1:.2f}%\")\n",
    "    if use_ema:\n",
    "        print(f\"        (Using EMA weights - {val_acc_ema:.2f}% vs {val_acc:.2f}%)\")\n",
    "    print(f\"        Precision: {precision.mean()*100:.2f}%, Recall: {recall.mean()*100:.2f}%\")\n",
    "    print(f\"        LR: {optimizer.param_groups[0]['lr']:.2e}, Time: {time.time()-epoch_start:.1f}s\")\n",
    "    \n",
    "    # Save best model (both standard and EMA)\n",
    "    if final_val_acc > best_val_acc:\n",
    "        best_val_acc = final_val_acc\n",
    "        best_val_f1 = final_val_f1\n",
    "        if use_ema:\n",
    "            ema.apply_shadow()\n",
    "            torch.save(classifier.state_dict(), '/kaggle/working/best_classifier.pth')\n",
    "            ema.restore()\n",
    "        else:\n",
    "            torch.save(classifier.state_dict(), '/kaggle/working/best_classifier.pth')\n",
    "        print(f\"‚úì Saved best model (acc: {best_val_acc:.2f}%, F1: {best_val_f1:.2f}%)\")\n",
    "    \n",
    "    # Comprehensive visualizations every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        plot_confusion_matrix(val_labels_ema if use_ema else val_labels, final_preds, epoch + 1)\n",
    "        plot_per_class_performance(precision, recall, f1_per_class, support)\n",
    "        plot_training_curves(history)\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Best Val F1 Score: {best_val_f1:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final comprehensive evaluation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION WITH BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best model\n",
    "classifier.load_state_dict(torch.load('/kaggle/working/best_classifier.pth'))\n",
    "val_loss, val_acc, val_f1, val_preds, val_labels = evaluate(classifier, val_loader, \n",
    "                                                             criterion, device)\n",
    "\n",
    "# Compute all metrics\n",
    "precision, recall, f1_per_class, support = compute_per_class_metrics(\n",
    "    val_labels, val_preds, config['num_classes']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Overall Metrics:\")\n",
    "print(f\"  Accuracy:  {val_acc:.2f}%\")\n",
    "print(f\"  F1 (Macro): {val_f1:.2f}%\")\n",
    "print(f\"  Precision: {precision.mean()*100:.2f}%\")\n",
    "print(f\"  Recall:    {recall.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìà Per-Class Statistics:\")\n",
    "print(f\"  Best F1 Class:   Class {f1_per_class.argmax()} ({f1_per_class.max()*100:.2f}%)\") \n",
    "print(f\"  Worst F1 Class:  Class {f1_per_class.argmin()} ({f1_per_class.min()*100:.2f}%)\")\n",
    "print(f\"  F1 Std Dev:      {f1_per_class.std()*100:.2f}%\")\n",
    "\n",
    "# Generate final visualizations\n",
    "plot_confusion_matrix(val_labels, val_preds, 'FINAL')\n",
    "plot_per_class_performance(precision, recall, f1_per_class, support)\n",
    "plot_training_curves(history)\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, digits=4))\n",
    "\n",
    "# Save classification report\n",
    "report = classification_report(val_labels, val_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv('/kaggle/working/classification_report.csv')\n",
    "print(\"\\n‚úì Saved classification report to classification_report.csv\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\n‚úì Training pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
