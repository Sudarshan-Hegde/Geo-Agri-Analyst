{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T14:19:38.304486Z",
     "iopub.status.busy": "2025-10-26T14:19:38.303827Z",
     "iopub.status.idle": "2025-10-26T14:23:08.875721Z",
     "shell.execute_reply": "2025-10-26T14:23:08.874966Z",
     "shell.execute_reply.started": "2025-10-26T14:19:38.304459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies installed successfully.\n",
      "W&B login successful.\n",
      "Using device: cuda with 2 GPUs (DataParallel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sea-9</strong> at: <a href='https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003/runs/srb9wsb5' target=\"_blank\">https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003/runs/srb9wsb5</a><br> View project at: <a href='https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003' target=\"_blank\">https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251117_061342-srb9wsb5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251117_061523-om7oppre</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003/runs/om7oppre' target=\"_blank\">flowing-wave-10</a></strong> to <a href='https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003' target=\"_blank\">https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003/runs/om7oppre' target=\"_blank\">https://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003/runs/om7oppre</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. WandB run 'flowing-wave-10' started.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install correct dependencies\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install --upgrade pip setuptools wheel --no-cache-dir\n",
    "!pip install -q torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121 --no-cache-dir --no-build-isolation\n",
    "!pip install -q lpips --no-deps --no-cache-dir\n",
    "!pip install -q basicsr facexlib gfpgan --no-cache-dir --no-build-isolation\n",
    "!pip install -q wandb umap-learn scikit-image rasterio pandas\n",
    "print(\"Dependencies installed successfully.\")\n",
    "\n",
    "# Step 2: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import time  # Added for timing\n",
    "import gc  # Added for memory\n",
    "from torch.nn.utils import clip_grad_norm_  # Added for clipping\n",
    "\n",
    "# Step 3: Login to WandB\n",
    "try:\n",
    "    wandb.login(key=\"5424a3d65aac1662f5be82d4439aaac35046689e\")\n",
    "    print(\"W&B login successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"W&B login failed: {e}. Please log in manually.\")\n",
    "    wandb.login()\n",
    "\n",
    "# Step 4: Setup Devices and Config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"Using device: {device}\" + (f\" with {gpu_count} GPUs (DataParallel).\" if gpu_count > 1 else \".\"))\n",
    "\n",
    "config = {\n",
    "    'project_name': 'SR-AL-pipeline-train-0003',\n",
    "    'dataset_size': 75000,  # Subset for 12h limit\n",
    "    'sr_epochs_psnr': 8,\n",
    "    'sr_epochs_gan': 10,  # Reduced for stability\n",
    "    'batch_size': 8,  # OOM-safe\n",
    "    'accum_steps': 8,  # Higher for fewer steps\n",
    "    'al_cycles': 4,\n",
    "    'al_epochs': 10,\n",
    "    'num_classes': 19,  # BigEarthNet-19\n",
    "    'lambda_perc': 5.0,  # Lower for less memory\n",
    "    'g_lr': 1e-4,\n",
    "    'd_lr': 1e-4,\n",
    "    'sr_psnr_lr': 2e-4\n",
    "}\n",
    "\n",
    "# Step 5: WandB Init\n",
    "wandb.init(project=config['project_name'], config=config)\n",
    "print(f\"Setup complete. WandB run '{wandb.run.name}' started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **cell 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T14:29:40.829309Z",
     "iopub.status.busy": "2025-10-26T14:29:40.828711Z",
     "iopub.status.idle": "2025-10-26T14:34:47.917445Z",
     "shell.execute_reply": "2025-10-26T14:34:47.916710Z",
     "shell.execute_reply.started": "2025-10-26T14:29:40.829281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 347244 band files in /kaggle/input/bigearthnetv2-s2-4/.\n",
      "Valid patches with RGB bands: 28937 (target 75000)\n",
      "Warning: Fewer valid patches than expected; check dataset for missing bands.\n",
      "Warning: metadata.parquet not found; using dummy multi-labels.\n",
      "Train patches: 23149, Val: 5788\n",
      "Dataset ready—PIL-based fixed transforms + Multi-Labels from metadata + Contrast Boost.\n"
     ]
    }
   ],
   "source": [
    "# Dataset: BigEarthNet v2 S2-4 (Single-band TIFFs per patch; load RGB as B02,B03,B04 + Multi-Labels)\n",
    "import numpy as np\n",
    "from PIL import Image  # For PIL augs\n",
    "from scipy.ndimage import zoom  # For NumPy bicubic downsample\n",
    "import pandas as pd  # For metadata\n",
    "from skimage import exposure  # For histogram eq (contrast boost)\n",
    "\n",
    "image_root_path = '/kaggle/input/bigearthnetv2-s2-4/'  # Your path\n",
    "ALL_TIF_PATHS = glob.glob(os.path.join(image_root_path, '**/*.tif'), recursive=True)\n",
    "\n",
    "print(f\"Found {len(ALL_TIF_PATHS)} band files in {image_root_path}.\")\n",
    "\n",
    "# Extract unique patch IDs (e.g., \"S2B_MSIL2A_20180421T100029_N9999_R122_T33TWM_00_00\" from filename)\n",
    "patch_to_bands = {}\n",
    "for path in ALL_TIF_PATHS:\n",
    "    fname = os.path.basename(path)\n",
    "    if fname.endswith('.tif'):\n",
    "        # Parse: everything before _Bxx.tif\n",
    "        if '_B' in fname:\n",
    "            patch_id = '_'.join(fname.split('_B')[:-1])\n",
    "            band = fname.split('_B')[-1].split('.')[0]  # '02', '03', etc.\n",
    "            if patch_id not in patch_to_bands:\n",
    "                patch_to_bands[patch_id] = {}\n",
    "            patch_to_bands[patch_id][band] = path\n",
    "\n",
    "# Sample 75k patches with all 3 RGB bands (B02, B03, B04)\n",
    "valid_patches = [pid for pid, bands in patch_to_bands.items() \n",
    "                 if all(b in bands for b in ['02', '03', '04'])]\n",
    "valid_patches = valid_patches[:config['dataset_size']]  # Subset if more\n",
    "\n",
    "print(f\"Valid patches with RGB bands: {len(valid_patches)} (target {config['dataset_size']})\")\n",
    "\n",
    "if len(valid_patches) < config['dataset_size'] // 2:\n",
    "    print(\"Warning: Fewer valid patches than expected; check dataset for missing bands.\")\n",
    "\n",
    "# Load Metadata for Multi-Labels (19 classes)\n",
    "metadata_path = os.path.join(image_root_path, 'metadata.parquet')  # Standard for BigEarthNet v2\n",
    "if os.path.exists(metadata_path):\n",
    "    df = pd.read_parquet(metadata_path)\n",
    "    # Assume 'patch_id' and 'labels' columns (list of int 0-18)\n",
    "    patch_to_label = {}\n",
    "    for _, row in df.iterrows():\n",
    "        pid = row['patch_id']  # Adjust column if different (e.g., 'patch_filename')\n",
    "        labels_list = row['labels'] if isinstance(row['labels'], list) else []  # List of class IDs\n",
    "        multi_hot = np.zeros(config['num_classes'])\n",
    "        for lbl in labels_list:\n",
    "            if 0 <= lbl < config['num_classes']:\n",
    "                multi_hot[lbl] = 1.0\n",
    "        if pid in valid_patches:\n",
    "            patch_to_label[pid] = torch.tensor(multi_hot, dtype=torch.float32)\n",
    "    print(f\"Loaded labels for {len(patch_to_label)} patches from metadata.\")\n",
    "else:\n",
    "    print(\"Warning: metadata.parquet not found; using dummy multi-labels.\")\n",
    "    patch_to_label = {pid: torch.full((config['num_classes'],), 1.0 / config['num_classes'], dtype=torch.float32) for pid in valid_patches}  # Uniform dummy\n",
    "\n",
    "# Spatial Aug Transforms (for PIL only)\n",
    "spatial_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),  # Multiples of 90° for orthoimages\n",
    "])\n",
    "\n",
    "# Tensor Transforms (post-spatial, for HWC NumPy)\n",
    "tensor_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # To [-1,1] for tanh output\n",
    "])\n",
    "\n",
    "# SR Dataset Class (PIL for spatial augs on HR → NumPy downsample to LR → Tensor + Label)\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, patch_ids, scale=4, patch_to_label=None):\n",
    "        self.patch_ids = patch_ids\n",
    "        self.scale = scale\n",
    "        self.patch_to_label = patch_to_label or {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patch_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patch_id = self.patch_ids[idx]\n",
    "        bands = {\n",
    "            'B02': patch_to_bands[patch_id].get('02'),  # Blue\n",
    "            'B03': patch_to_bands[patch_id].get('03'),  # Green\n",
    "            'B04': patch_to_bands[patch_id].get('04')   # Red\n",
    "        }\n",
    "        \n",
    "        # Label (multi-hot)\n",
    "        label = self.patch_to_label.get(patch_id, torch.zeros(config['num_classes'], dtype=torch.float32))\n",
    "        \n",
    "        try:\n",
    "            if not all(bands.values()):\n",
    "                raise ValueError(\"Missing RGB band\")\n",
    "            \n",
    "            # Read bands (120x120 uint16 → float [0,1])\n",
    "            b02 = rasterio.open(bands['B02']).read(1, masked=True).astype(np.float32) / 10000.0\n",
    "            b03 = rasterio.open(bands['B03']).read(1, masked=True).astype(np.float32) / 10000.0\n",
    "            b04 = rasterio.open(bands['B04']).read(1, masked=True).astype(np.float32) / 10000.0\n",
    "            \n",
    "            # Stack to (H,W,3) RGB; fill masked (NaN/clouds) with 0\n",
    "            hr_np = np.stack([b04, b03, b02], axis=-1)  # RGB order (B04=Red, B03=Green, B02=Blue)\n",
    "            if np.ma.is_masked(hr_np):\n",
    "                hr_np = hr_np.filled(0.0)\n",
    "            hr_np = np.clip(hr_np, 0, 1)  # [0,1]\n",
    "            \n",
    "            # FIX: Boost contrast with adaptive histogram eq (per-channel for dark Sentinel-2)\n",
    "            hr_np = exposure.equalize_adapthist(hr_np)\n",
    "            hr_np = np.clip(hr_np, 0, 1)\n",
    "            \n",
    "            # Convert HR NumPy to PIL Image (HWC uint8)\n",
    "            hr_pil = Image.fromarray((hr_np * 255).astype(np.uint8))\n",
    "            \n",
    "            # Apply spatial augs to PIL HR (random state per sample)\n",
    "            hr_aug_pil = spatial_transforms(hr_pil)\n",
    "            \n",
    "            # Back to NumPy [0,1] HWC\n",
    "            hr_aug_np = np.array(hr_aug_pil).astype(np.float32) / 255.0\n",
    "            hr_aug_np = np.clip(hr_aug_np, 0, 1)\n",
    "            \n",
    "            # LR: Downsample augmented HR NumPy to x4 smaller (bicubic via zoom)\n",
    "            # Transpose to CHW for zoom (operates on spatial dims)\n",
    "            hr_chw = hr_aug_np.transpose(2, 0, 1)  # (3,120,120)\n",
    "            lr_chw = zoom(hr_chw, (1, 1/self.scale, 1/self.scale), order=3)  # (3,30,30)\n",
    "            lr_np = lr_chw.transpose(1, 2, 0)  # Back to HWC (30,30,3)\n",
    "            lr_np = np.clip(lr_np, 0, 1)\n",
    "            \n",
    "            # To Tensor + Normalize: Pass HWC NumPy directly (ToTensor handles transpose)\n",
    "            hr = tensor_transforms(hr_aug_np)\n",
    "            lr = tensor_transforms(lr_np)\n",
    "            \n",
    "            return {'lr': lr, 'hr': hr, 'label': label}\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Skip bad patch; return black dummy as normalized Tensor + zero label\n",
    "            print(f\"Error loading patch {patch_id}: {e}\")\n",
    "            dummy_size_hr, dummy_size_lr = 120, 30\n",
    "            # HWC [0,1] black\n",
    "            dummy_black_hr = np.zeros((dummy_size_hr, dummy_size_hr, 3))\n",
    "            dummy_black_lr = np.zeros((dummy_size_lr, dummy_size_lr, 3))\n",
    "            # Direct to transforms (no transpose)\n",
    "            dummy_hr = tensor_transforms(dummy_black_hr)\n",
    "            dummy_lr = tensor_transforms(dummy_black_lr)\n",
    "            return {'lr': dummy_lr, 'hr': dummy_hr, 'label': torch.zeros(config['num_classes'], dtype=torch.float32)}\n",
    "\n",
    "# Split and Loaders (80/20, random for multi-label)\n",
    "train_ids, val_ids = train_test_split(valid_patches, test_size=0.2, random_state=42)\n",
    "train_ds = SRDataset(train_ids, patch_to_label=patch_to_label)\n",
    "val_ds = SRDataset(val_ids, patch_to_label=patch_to_label)\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train patches: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "print(\"Dataset ready—PIL-based fixed transforms + Multi-Labels from metadata + Contrast Boost.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: LR torch.Size([3, 30, 30]), HR torch.Size([3, 120, 120]), Range LR: [-1.00, 0.90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:326: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 2.0 + (arr[idx, 2] - arr[idx, 0]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: LR torch.Size([8, 3, 30, 30]), HR torch.Size([8, 3, 120, 120])\n"
     ]
    }
   ],
   "source": [
    "# Test single item\n",
    "sample = train_ds[0]\n",
    "print(f\"Shapes: LR {sample['lr'].shape}, HR {sample['hr'].shape}, Range LR: [{sample['lr'].min():.2f}, {sample['lr'].max():.2f}]\")\n",
    "# Test batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch shapes: LR {batch['lr'].shape}, HR {batch['hr'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *cell 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T14:41:58.240336Z",
     "iopub.status.busy": "2025-10-26T14:41:58.239662Z",
     "iopub.status.idle": "2025-10-26T14:42:00.182316Z",
     "shell.execute_reply": "2025-10-26T14:42:00.181710Z",
     "shell.execute_reply.started": "2025-10-26T14:41:58.240312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "# Models: RFB-ESRGAN Generator, RelDiscriminator, PerceptualLoss\n",
    "# (Based on ESRGAN; assume growth=32 for lightweight)\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, nc=64, growth=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(nc, growth, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nc + growth, growth, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nc + 2*growth, growth, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nc + 3*growth, growth, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nc + 4*growth, nc, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat([x, x1], 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat([x, x1, x2], 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat([x, x1, x2, x3], 1)))\n",
    "        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], 1))\n",
    "        # FIX: Light anti-aliasing blur to reduce checkerboard in residuals\n",
    "        x5 = F.avg_pool2d(x5, kernel_size=3, stride=1, padding=1)\n",
    "        return x5 * 0.2 + x  # Dense residual\n",
    "\n",
    "class RFBESRGANGenerator(nn.Module):\n",
    "    def __init__(self, growth=32, num_blocks=23, nc=64, upscale=4):\n",
    "        super().__init__()\n",
    "        self.entry = nn.Sequential(nn.Conv2d(3, nc, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.body = nn.ModuleList([ResidualDenseBlock(nc, growth) for _ in range(num_blocks)])\n",
    "        self.conv_tail = nn.Conv2d(nc, nc, 3, 1, 1)\n",
    "        # FIX: Bilinear upsample + Conv (replaces TransposedConv to avoid checkerboard artifacts)\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(nc, nc//2, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(nc//2, 3, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry(x)\n",
    "        res = x\n",
    "        for block in self.body:\n",
    "            x = block(x)\n",
    "        x = self.conv_tail(x)\n",
    "        x += res  # Global skip\n",
    "        x = self.up(x)\n",
    "        return torch.tanh(x)  # [-1,1] range\n",
    "\n",
    "class RelDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0)  # PatchGAN output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True).features[:35].eval()\n",
    "        for p in vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        # Ensure inputs on same device as module\n",
    "        sr = sr.to(self.vgg.device)\n",
    "        hr = hr.to(self.vgg.device)\n",
    "        sr_vgg = (self.vgg((sr + 1)/2 * self.std + self.mean) + 1)/2\n",
    "        hr_vgg = (self.vgg((hr + 1)/2 * self.std + self.mean) + 1)/2\n",
    "        return self.l1(sr_vgg, hr_vgg)\n",
    "\n",
    "# PSNR/SSIM Helpers (FIX: Smaller win_size for small images)\n",
    "def psnr(sr, hr, max_val=1.0):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    return 20 * math.log10(max_val / math.sqrt(mse))\n",
    "\n",
    "def compute_ssim(sr, hr):\n",
    "    # FIX: win_size=3 for 30x30 LR; channel_axis=-1 for HWC RGB\n",
    "    return ssim(sr.permute(1,2,0).cpu().numpy(), hr.permute(1,2,0).cpu().numpy(), \n",
    "                multichannel=True, channel_axis=-1, data_range=1.0, win_size=3)\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T15:19:42.119040Z",
     "iopub.status.busy": "2025-10-26T15:19:42.118312Z",
     "iopub.status.idle": "2025-10-26T15:19:42.202061Z",
     "shell.execute_reply": "2025-10-26T15:19:42.201193Z",
     "shell.execute_reply.started": "2025-10-26T15:19:42.119014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 4 using: cuda with 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:02<00:00, 203MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models instantiated.\n",
      "Starting PSNR pre-training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4daccea5eb4a12876cf3422968acd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 1/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88cb7c21fbb4436af284250f6c47e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 2/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18f83cdbcfb49a090b17fa39227b644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 3/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0613eb3c0bf44f618eb6576b8237f0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 4/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a4e981316b42de9e2b4df991cb0f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 5/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /kaggle/working/g_psnr_ep4.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13106b2ecfc54702b4c0849ecebfc860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 6/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82def8f827ff4a25aac2c4870baf577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 7/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cb6b10b4ac4fbe81310f2eb045be3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PSNR Ep 8/8:   0%|          | 0/2894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:322: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = (arr[idx, 1] - arr[idx, 2]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n",
      "/usr/local/lib/python3.11/dist-packages/skimage/color/colorconv.py:330: RuntimeWarning: divide by zero encountered in divide\n",
      "  out[idx, 0] = 4.0 + (arr[idx, 0] - arr[idx, 1]) / delta[idx]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/1227776267.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;31m# --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting PSNR pre-training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_psnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sr_epochs_psnr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PSNR pre-training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_38/1227776267.py\u001b[0m in \u001b[0;36mtrain_psnr\u001b[0;34m(g, loader, epochs, lr)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mwandering-sky-1\u001b[0m at: \u001b[34mhttps://wandb.ai/hegdesudarshan-hegde/SR-AL-pipeline-train-0003/runs/8jy2jd5m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251114_132829-8jy2jd5m/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# SECTION 4: SR MODEL TRAINING\n",
    "# (Two-stage: PSNR + GAN, with fixes: clipping, mem mgmt, error handling)\n",
    "# --------------------------------------------------------------------------------\n",
    "print(f\"Cell 4 using: {device} with {gpu_count} GPUs.\")\n",
    "\n",
    "# Instantiate Models\n",
    "g = RFBESRGANGenerator(growth=32).to(device)\n",
    "if gpu_count > 1:\n",
    "    g = nn.DataParallel(g)\n",
    "wandb.watch(g, log=\"all\", log_freq=100)\n",
    "\n",
    "d = RelDiscriminator().to(device)\n",
    "if gpu_count > 1:\n",
    "    d = nn.DataParallel(d)\n",
    "wandb.watch(d, log=\"all\", log_freq=100)\n",
    "\n",
    "perc_loss = PerceptualLoss().to(device)  # FIX: Move to device to resolve CPU/GPU mismatch\n",
    "print(\"Models instantiated.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# --------------------------------------------------------------------------------\n",
    "def save_model(model, path):\n",
    "    \"\"\"Saves model state_dict, handling DataParallel.\"\"\"\n",
    "    try:\n",
    "        state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "        torch.save(state_dict, path)\n",
    "        wandb.save(path, policy=\"now\")\n",
    "        print(f\"Saved {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Save error: {e}\"); wandb.log({\"error\": str(e)})\n",
    "\n",
    "def log_sr_samples(g, val_loader, epoch, phase):\n",
    "    g.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            batch = next(iter(val_loader))\n",
    "        except StopIteration:\n",
    "            print(\"Could not get validation batch for logging.\")\n",
    "            return\n",
    "           \n",
    "        lr_sample, hr_sample = batch['lr'][:4].to(device), batch['hr'][:4].to(device)\n",
    "       \n",
    "        model_to_run = g.module if gpu_count > 1 else g\n",
    "        with autocast():\n",
    "            sr_sample = model_to_run(lr_sample)\n",
    "       \n",
    "        fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "        for i in range(4):\n",
    "            # FIX: Denorm [-1,1] to [0,1] before permute/clamp\n",
    "            lr_img = (lr_sample[i] + 1) / 2\n",
    "            sr_img = (sr_sample[i] + 1) / 2\n",
    "            hr_img = (hr_sample[i] + 1) / 2\n",
    "            \n",
    "            lr_img_np = lr_img.permute(1,2,0).cpu().clamp(0,1).float().numpy()\n",
    "            sr_img_np = sr_img.permute(1,2,0).cpu().clamp(0,1).float().numpy()\n",
    "            hr_img_np = hr_img.permute(1,2,0).cpu().clamp(0,1).float().numpy()\n",
    "           \n",
    "            axes[i,0].imshow(lr_img_np); axes[i,0].set_title('LR'); axes[i,0].axis('off')\n",
    "            axes[i,1].imshow(sr_img_np); axes[i,1].set_title('SR'); axes[i,1].axis('off')\n",
    "            axes[i,2].imshow(hr_img_np); axes[i,2].set_title('HR'); axes[i,2].axis('off')\n",
    "           \n",
    "        plt.suptitle(f\"{phase} Samples - Epoch {epoch}\")\n",
    "        wandb.log({f\"SR_{phase}/samples\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "# PSNR Pretraining (unchanged, stable)\n",
    "def train_psnr(g, loader, epochs, lr=2e-4):\n",
    "    opt = optim.Adam(g.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    l1 = nn.L1Loss().to(device)\n",
    "    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    for ep in range(epochs):\n",
    "        start_time = time.time()\n",
    "        g.train()\n",
    "        tot_psnr, tot_ssim, tot_loss, num_samples = 0, 0, 0, 0\n",
    "        pbar = tqdm(loader, desc=f\"PSNR Ep {ep+1}/{epochs}\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            lr_imgs, hr_imgs = batch['lr'].to(device), batch['hr'].to(device)\n",
    "            batch_size = lr_imgs.shape[0]\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                sr = g(lr_imgs)\n",
    "                loss = l1(sr, hr_imgs)\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i + 1) % config['accum_steps'] == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad()\n",
    "               \n",
    "            tot_loss += loss.item() * batch_size\n",
    "            current_psnr = psnr(sr, hr_imgs)\n",
    "            tot_psnr += current_psnr * batch_size\n",
    "           \n",
    "            try:\n",
    "                for b in range(batch_size):\n",
    "                    tot_ssim += compute_ssim(sr[b], hr_imgs[b])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "           \n",
    "            num_samples += batch_size\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"PSNR\": f\"{current_psnr:.2f}\"})\n",
    "            \n",
    "            # Mem cleanup\n",
    "            if i % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "       \n",
    "        avg_loss = tot_loss / num_samples\n",
    "        avg_psnr = tot_psnr / num_samples\n",
    "        avg_ssim = tot_ssim / num_samples if num_samples > 0 else 0\n",
    "        sched.step()\n",
    "       \n",
    "        epoch_time = time.time() - start_time\n",
    "        wandb.log({\"SR_PSNR/epoch\": ep, \"SR_PSNR/loss\": avg_loss, \"SR_PSNR/psnr\": avg_psnr, \n",
    "                   \"SR_PSNR/ssim\": avg_ssim, \"SR_PSNR/epoch_time\": epoch_time})\n",
    "       \n",
    "        if (ep + 1) % 5 == 0 or ep == epochs - 1:\n",
    "            log_sr_samples(g, val_loader, ep, \"PSNR\")\n",
    "            save_model(g, f'/kaggle/working/g_psnr_ep{ep}.pth')\n",
    "   \n",
    "    return g\n",
    "\n",
    "# GAN Fine-Tuning (with fixes: clipping, mem, error handling, reduced val)\n",
    "def train_gan(g, d, loader, epochs, g_lr=1e-4, lambda_perc=10.0):\n",
    "    g_opt = optim.Adam(g.parameters(), lr=g_lr)\n",
    "    d_opt = optim.Adam(d.parameters(), lr=config['d_lr'])\n",
    "    scaler_g = GradScaler()\n",
    "    scaler_d = GradScaler()\n",
    "    sched_g = optim.lr_scheduler.CosineAnnealingLR(g_opt, T_max=epochs)\n",
    "    sched_d = optim.lr_scheduler.CosineAnnealingLR(d_opt, T_max=epochs)\n",
    "    adv = nn.BCEWithLogitsLoss().to(device)\n",
    "    l1 = nn.L1Loss().to(device)\n",
    "   \n",
    "    for ep in range(epochs):\n",
    "        start_time = time.time()\n",
    "        g.train(); d.train()\n",
    "        tot_g_loss, tot_d_loss, num_samples_g, num_samples_d = 0, 0, 0, 0\n",
    "        pbar = tqdm(loader, desc=f\"GAN Ep {ep+1}/{epochs}\")\n",
    "        \n",
    "        # Mem log\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"Epoch {ep}: GPU mem {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
    "        \n",
    "        for i, batch in enumerate(pbar):\n",
    "            lr_imgs, hr_imgs = batch['lr'].to(device), batch['hr'].to(device)\n",
    "            batch_size = lr_imgs.shape[0]\n",
    "           \n",
    "            # --- Train Discriminator ---\n",
    "            try:\n",
    "                d_opt.zero_grad(set_to_none=True)\n",
    "                with autocast():\n",
    "                    real_pred = d(hr_imgs)\n",
    "                    fake = g(lr_imgs)\n",
    "                    fake_pred = d(fake.detach())\n",
    "               \n",
    "                    d_loss_real = adv(real_pred - fake_pred.mean(), torch.ones_like(real_pred))\n",
    "                    d_loss_fake = adv(fake_pred - real_pred.mean(), torch.zeros_like(fake_pred))\n",
    "                    d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "                    d_loss = d_loss / config['accum_steps']\n",
    "               \n",
    "                scaler_d.scale(d_loss).backward()\n",
    "                clip_grad_norm_(d.parameters(), max_norm=1.0)  # FIX: Clip\n",
    "                tot_d_loss += d_loss.item() * config['accum_steps'] * batch_size\n",
    "                num_samples_d += batch_size\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"OOM in D at batch {i}\"); torch.cuda.empty_cache(); continue\n",
    "                raise\n",
    "           \n",
    "            # --- Train Generator ---\n",
    "            try:\n",
    "                g_opt.zero_grad(set_to_none=True)\n",
    "                with autocast():\n",
    "                    fake = g(lr_imgs)\n",
    "                    real_pred_g = d(hr_imgs).detach()\n",
    "                    fake_pred_g = d(fake)\n",
    "               \n",
    "                    g_adv = (adv(fake_pred_g - real_pred_g.mean(), torch.ones_like(fake_pred_g)) +\n",
    "                             adv(real_pred_g.mean() - fake_pred_g, torch.zeros_like(real_pred_g))) / 2\n",
    "               \n",
    "                    g_perc = perc_loss(fake, hr_imgs)\n",
    "                    g_l1 = l1(fake, hr_imgs)\n",
    "                    g_loss = 0.001 * g_adv + lambda_perc * g_perc + g_l1\n",
    "                    g_loss = g_loss / config['accum_steps']\n",
    "                scaler_g.scale(g_loss).backward()\n",
    "                clip_grad_norm_(g.parameters(), max_norm=1.0)  # FIX: Clip\n",
    "                tot_g_loss += g_loss.item() * config['accum_steps'] * batch_size\n",
    "                num_samples_g += batch_size\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"OOM in G at batch {i}\"); torch.cuda.empty_cache(); continue\n",
    "                raise\n",
    "           \n",
    "            # --- Optimizer Step ---\n",
    "            if (i + 1) % config['accum_steps'] == 0:\n",
    "                scaler_d.step(d_opt); scaler_d.update(); d_opt.zero_grad()\n",
    "                scaler_g.step(g_opt); scaler_g.update(); g_opt.zero_grad()\n",
    "           \n",
    "            pbar.set_postfix({\"G_Loss\": f\"{g_loss.item()*config['accum_steps']:.4f}\", \"D_Loss\": f\"{d_loss.item()*config['accum_steps']:.4f}\"})\n",
    "            \n",
    "            # Mem cleanup every 5 batches\n",
    "            if i % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "       \n",
    "        avg_g_loss = tot_g_loss / num_samples_g if num_samples_g > 0 else 0\n",
    "        avg_d_loss = tot_d_loss / num_samples_d if num_samples_d > 0 else 0\n",
    "        sched_g.step(); sched_d.step()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "       \n",
    "        # --- Quick Validation (reduced: 2 batches, skip SSIM if error) ---\n",
    "        avg_psnr, avg_ssim, val_images = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "            val_iter = iter(val_loader)\n",
    "            num_batches = 2  # FIX: Reduced\n",
    "            for _ in range(num_batches):\n",
    "                try:\n",
    "                    batch = next(val_iter)\n",
    "                    lr_v, hr_v = batch['lr'].to(device), batch['hr'].to(device)\n",
    "                    val_batch_size = lr_v.shape[0]\n",
    "                    with autocast():\n",
    "                        sr_v = g(lr_v) if gpu_count == 1 else g.module(lr_v)\n",
    "                   \n",
    "                    avg_psnr += psnr(sr_v, hr_v) * val_batch_size\n",
    "                   \n",
    "                    # SSIM with error skip\n",
    "                    try:\n",
    "                        for b in range(val_batch_size):\n",
    "                            sr_b = sr_v[b]\n",
    "                            hr_b = hr_v[b]\n",
    "                            avg_ssim += compute_ssim(sr_b, hr_b)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Val SSIM error: {e}\"); avg_ssim += 0\n",
    "                   \n",
    "                    val_images += val_batch_size\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        print(\"OOM in val\"); break\n",
    "                    raise\n",
    "       \n",
    "        if val_images > 0:\n",
    "            avg_psnr /= val_images\n",
    "            avg_ssim /= val_images\n",
    "       \n",
    "        wandb.log({\n",
    "            \"SR_GAN/epoch\": ep, \"SR_GAN/g_loss\": avg_g_loss, \"SR_GAN/d_loss\": avg_d_loss,\n",
    "            \"SR_GAN/psnr\": avg_psnr, \"SR_GAN/ssim\": avg_ssim, \"SR_GAN/epoch_time\": epoch_time\n",
    "        })\n",
    "       \n",
    "        if (ep + 1) % 5 == 0 or ep == epochs - 1:\n",
    "            log_sr_samples(g, val_loader, ep, \"GAN\")\n",
    "            save_model(g, f'/kaggle/working/g_gan_ep{ep}.pth')\n",
    "   \n",
    "    return g\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"Starting PSNR pre-training...\")\n",
    "g = train_psnr(g, train_loader, config['sr_epochs_psnr'])\n",
    "print(\"PSNR pre-training finished.\")\n",
    "\n",
    "\n",
    "print(\"\\nStarting GAN fine-tuning...\")\n",
    "g = train_gan(g, d, train_loader, config['sr_epochs_gan'], g_lr=config['g_lr'], lambda_perc=config['lambda_perc'])\n",
    "print(\"GAN fine-tuning finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final evaluation...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/2169797614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# We assume 'psnr' and 'compute_ssim' exist from Cell 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfinal_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_lbls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Calculate final PSNR/SSIM on a subset of the validation loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save Final SR Model\n",
    "save_model(g, '/kaggle/working/sr_model.pth')\n",
    "print(\"SR Model saved & logged to WandB.\")\n",
    "\n",
    "# Quick Eval: PSNR/SSIM on full val set\n",
    "g.eval()\n",
    "tot_psnr, tot_ssim, num = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Final Eval\"):\n",
    "        lr, hr = batch['lr'].to(device), batch['hr'].to(device)\n",
    "        with autocast():\n",
    "            sr = g(lr) if gpu_count == 1 else g.module(lr)\n",
    "        tot_psnr += psnr(sr, hr) * lr.shape[0]\n",
    "        try:\n",
    "            for b in range(lr.shape[0]):\n",
    "                tot_ssim += compute_ssim(sr[b], hr[b])\n",
    "        except:\n",
    "            pass\n",
    "        num += lr.shape[0]\n",
    "\n",
    "print(f\"Final Val: PSNR={tot_psnr/num:.2f}, SSIM={tot_ssim/num:.4f}\")\n",
    "wandb.log({\"final_psnr\": tot_psnr/num, \"final_ssim\": tot_ssim/num})\n",
    "wandb.finish()\n",
    "print(\"Run complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataset_root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38/1627722696.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# (FIXED) This is the correct path for the Kaggle EuroSAT dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# MAKE SURE YOU HAVE ADDED THE `eurosat` DATASET TO YOUR KAGGLE NOTEBOOK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_root'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Root path missing: {root_path}. Please add the EuroSAT dataset via '+ Add data'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset_root'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 1. DATASET CLASS FOR EUROSAT\n",
    "# --------------------------------------------------------------------------------\n",
    "class EuroSAT_SR_Dataset(Dataset):\n",
    "    def __init__(self, full_dataset, indices, scale=4, transform_hr=None, transform_lr=None, phase='train'):\n",
    "        self.full_dataset = full_dataset\n",
    "        self.indices = indices # The list of master indices this dataset should use\n",
    "        self.scale = scale\n",
    "        self.transform_hr = transform_hr\n",
    "        self.transform_lr = transform_lr\n",
    "        self.phase = phase\n",
    "        \n",
    "        # Base transforms\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.hr_resize = transforms.Resize((120, 120), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "        self.lr_resize = transforms.Resize((30, 30), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "        self.blur = transforms.GaussianBlur(kernel_size=3, sigma=0.5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the master index (e.g., 42_000) from our subset list\n",
    "        master_idx = self.indices[idx]\n",
    "        \n",
    "        # Get the PIL image and integer label from the base ImageFolder dataset\n",
    "        hr_img_pil, label = self.full_dataset[master_idx]\n",
    "        \n",
    "        # Apply HR augmentation (flips/rotations) if in SR training\n",
    "        if self.phase == 'train_sr' and self.transform_hr:\n",
    "            hr_img_pil = self.transform_hr(hr_img_pil)\n",
    "        \n",
    "        # Create LR image\n",
    "        if self.phase == 'train_sr' and self.transform_lr:\n",
    "            # For training, apply color jitter *to the HR image* before downscaling\n",
    "            lr_img_pil = self.transform_lr(hr_img_pil)\n",
    "        else:\n",
    "            # For validation/AL, just use the original PIL image\n",
    "            lr_img_pil = hr_img_pil\n",
    "        \n",
    "        # Apply final transforms\n",
    "        hr_tensor = self.to_tensor(self.hr_resize(hr_img_pil))\n",
    "        lr_tensor = self.to_tensor(self.blur(self.lr_resize(lr_img_pil)))\n",
    "\n",
    "        return {'lr': lr_tensor, 'hr': hr_tensor, 'label': torch.tensor(label, dtype=torch.long), 'idx': master_idx}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# 2. DATASET LOADING AND SPLITTING\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# (FIXED) This is the correct path for the Kaggle EuroSAT dataset\n",
    "# MAKE SURE YOU HAVE ADDED THE `eurosat` DATASET TO YOUR KAGGLE NOTEBOOK\n",
    "root_path = config['dataset_root']\n",
    "assert os.path.exists(root_path), f\"Root path missing: {root_path}. Please add the EuroSAT dataset via '+ Add data'.\"\n",
    "\n",
    "# Load the base dataset using ImageFolder. This *automatically* gets the labels.\n",
    "base_dataset = ImageFolder(root=root_path)\n",
    "all_class_names = base_dataset.classes # e.g., ['AnnualCrop', 'Forest', ...]\n",
    "all_labels = base_dataset.targets     # e.g., [0, 5, 2, 1, 0, 9, ...]\n",
    "print(f\"Found {len(base_dataset)} images in {len(all_class_names)} classes.\")\n",
    "print(f\"Classes: {all_class_names}\")\n",
    "\n",
    "# Update config with the *actual* number of classes\n",
    "if len(all_class_names) != config['num_classes']:\n",
    "    print(f\"WARNING: Config expected {config['num_classes']} classes, but found {len(all_class_names)}. Updating config.\")\n",
    "    config['num_classes'] = len(all_class_names)\n",
    "\n",
    "all_indices = list(range(len(all_labels)))\n",
    "\n",
    "# (CRITICAL FIX) Stratify on the *real* labels from the dataset\n",
    "train_idx, val_idx = train_test_split(\n",
    "    all_indices, \n",
    "    train_size=0.8, \n",
    "    random_state=42, \n",
    "    stratify=all_labels # This fixes all your label bugs\n",
    ")\n",
    "\n",
    "# Define transforms\n",
    "sr_lr_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    # Note: Resize and Blur are handled *inside* the dataset class\n",
    "])\n",
    "sr_hr_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15)\n",
    "])\n",
    "val_lr_transform = None\n",
    "val_hr_transform = None\n",
    "\n",
    "# Create Datasets\n",
    "train_ds = EuroSAT_SR_Dataset(base_dataset, train_idx, scale=4, lr_transform=sr_lr_transform, hr_transform=sr_hr_transform, phase='train_sr')\n",
    "val_ds = EuroSAT_SR_Dataset(base_dataset, val_idx, scale=4, phase='val')\n",
    "\n",
    "# This dataset will be used by the Active Learning loop (no augs)\n",
    "al_base_dataset = EuroSAT_SR_Dataset(base_dataset, train_idx, scale=4, phase='al')\n",
    "val_loader_al_dataset = EuroSAT_SR_Dataset(base_dataset, val_idx, scale=4, phase='al')\n",
    "\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "wandb.log({\"dataset/train_samples\": len(train_ds), \"dataset/val_samples\": len(val_ds), \"dataset/classes\": len(all_class_names)})\n",
    "\n",
    "print(f\"EuroSAT loaded: {len(train_ds)} train, {len(val_ds)} val samples\")\n",
    "print(\"Sample batch keys:\", next(iter(train_loader)).keys())\n",
    "print(\"Sample LR shape:\", next(iter(train_loader))['lr'].shape)\n",
    "print(\"Sample HR shape:\", next(iter(train_loader))['hr'].shape)\n",
    "\n",
    "# Store global variables for other cells\n",
    "# We need these for the AL loop in Cell 6\n",
    "al_train_indices = train_idx\n",
    "al_val_indices = val_idx\n",
    "al_all_labels = all_labels\n",
    "al_class_names = all_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-26T11:52:16.856232Z",
     "iopub.status.idle": "2025-10-26T11:52:16.856510Z",
     "shell.execute_reply": "2025-10-26T11:52:16.856398Z",
     "shell.execute_reply.started": "2025-10-26T11:52:16.856382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# SECTION 5: CLASSIFIER & ACTIVE LEARNING DEFINITIONS\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# (NEW) RobustClassifier with SE (ResNet-inspired)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # Squeeze\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c, c // r, bias=False), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Linear(c // r, c, bias=False), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        return x * y.view(x.shape[0], x.shape[1], 1, 1) # Excitation\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"ResNet-style block with SE\"\"\"\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.se = SEBlock(out_c)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride > 1 or in_c != out_c:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out) # Apply attention\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class RobustClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The main classifier model.\n",
    "    Takes 120x120 SR images as input (as trained in Cell 4).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_blocks=[3,4,6,3]):\n",
    "        super().__init__()\n",
    "        self.in_c = 64\n",
    "        # Initial 7x7 conv to reduce dimensions\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False); self.bn1 = nn.BatchNorm2d(64) # 120x120 -> 60x60\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1) # 60x60\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2) # 30x30\n",
    "        self.layer3 = self._make_layer(256, num_blocks[2], stride=2) # 15x15\n",
    "        self.layer4 = self._make_layer(512, num_blocks[3], stride=2) # 8x8\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, out_c, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_c, out_c, s))\n",
    "            self.in_c = out_c\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
    "        x = self.pool(x)\n",
    "        return self.fc(x.flatten(1))\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.pool(x).flatten(1)\n",
    "\n",
    "# AL Helpers\n",
    "def get_embeddings(clf, loader, sr_model):\n",
    "    clf.eval(); sr_model.eval()\n",
    "    embeds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Extracting Embeds\"):\n",
    "            with autocast():\n",
    "                imgs = sr_model(batch['lr'].to(device))\n",
    "            # (FIXED) Use gpu_count, not world_size\n",
    "            emb = clf.module.get_embeddings(imgs) if gpu_count > 1 else clf.get_embeddings(imgs)\n",
    "            embeds.append(emb.cpu().numpy())\n",
    "            labels.append(batch['label'].numpy())\n",
    "    return np.concatenate(embeds), np.concatenate(labels)\n",
    "\n",
    "def dbss_select(unlabeled_embs, labeled_embs, labels, n_select, pin_ratio=0.4):\n",
    "    num_classes = config['num_classes']\n",
    "    centroids = np.zeros((num_classes, labeled_embs.shape[1]))\n",
    "    # Calculate centroids, handling classes that might not be in the labeled set yet\n",
    "    for i in range(num_classes):\n",
    "        class_samples = labeled_embs[labels == i]\n",
    "        if len(class_samples) > 0:\n",
    "            centroids[i] = class_samples.mean(axis=0)\n",
    "        else:\n",
    "            centroids[i] = labeled_embs.mean(axis=0) # Fallback\n",
    "            \n",
    "    dists = np.linalg.norm(unlabeled_embs[:, None] - centroids[None], axis=2)\n",
    "    \n",
    "    # Inner-class samples (high score = far from all, uncertain)\n",
    "    inner_scores = np.sum(np.log(dists + 1e-6), axis=1)\n",
    "    inner_rank = np.argsort(inner_scores)[::-1] # High score is good\n",
    "    \n",
    "    # Border samples (low score = on border)\n",
    "    dists.sort(axis=1)\n",
    "    border_scores = np.abs(dists[:, 0] - dists[:, 1])\n",
    "    border_rank = np.argsort(border_scores) # Low score is good\n",
    "    \n",
    "    selected = set(inner_rank[:int(n_select * pin_ratio)])\n",
    "    for idx in border_rank:\n",
    "        if len(selected) >= n_select:\n",
    "            break\n",
    "        selected.add(idx)\n",
    "    return list(selected)\n",
    "\n",
    "def ssas_pseudo(student, teacher, sr_model, unlabeled_loader):\n",
    "    student.eval(); teacher.eval(); sr_model.eval()\n",
    "    consistent_indices = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(unlabeled_loader, desc=\"SSAS\"):\n",
    "            with autocast():\n",
    "                imgs = sr_model(batch['lr'].to(device))\n",
    "                s_pred = torch.argmax(student(imgs), 1)\n",
    "                t_pred = torch.argmax(teacher(imgs), 1)\n",
    "            mask = s_pred == t_pred\n",
    "            consistent_indices.extend(batch['idx'][mask].tolist()) # Get original master indices\n",
    "    return consistent_indices\n",
    "\n",
    "def train_classifier(clf, loader, epochs):\n",
    "    clf.train()\n",
    "    opt = optim.Adam(clf.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        tot_loss = 0\n",
    "        pbar = tqdm(loader, desc=f\"CLF Ep {ep+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            # We train the classifier on the SR-enhanced images\n",
    "            imgs, lbls = g(batch['lr'].to(device)), batch['label'].to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = clf(imgs)\n",
    "                loss = criterion(outputs, lbls)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            tot_loss += loss.item()\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "        # (FIXED) Removed local_rank error\n",
    "        wandb.log({\"AL_Train/loss\": tot_loss / len(loader)})\n",
    "    return clf\n",
    "\n",
    "def evaluate_model(clf, loader, sr_model=None):\n",
    "    clf.eval()\n",
    "    preds, lbls = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Eval\"):\n",
    "            with autocast():\n",
    "                imgs = sr_model(batch['lr'].to(device)) if sr_model else batch['lr'].to(device)\n",
    "                outputs = clf(imgs)\n",
    "            # (FIXED) Use argmax for single-label, matching CrossEntropyLoss\n",
    "            pred = torch.argmax(outputs, 1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            lbls.extend(batch['label'].numpy())\n",
    "    acc = (np.array(preds) == np.array(lbls)).mean() * 100\n",
    "    return acc, preds, lbls\n",
    "\n",
    "def log_umap(labeled_embs, labeled_lbls, unlabeled_embs, dbss_idx, ssas_idx, cycle):\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42, n_jobs=1)\n",
    "    \n",
    "    if len(unlabeled_embs) == 0:\n",
    "        print(\"No unlabeled samples left to plot in UMAP.\")\n",
    "        return\n",
    "\n",
    "    all_embs = np.concatenate([labeled_embs, unlabeled_embs])\n",
    "    all_lbls = np.concatenate([labeled_lbls, -1 * np.ones(len(unlabeled_embs))])\n",
    "    \n",
    "    print(\"Fitting UMAP...\")\n",
    "    emb_2d = reducer.fit_transform(all_embs)\n",
    "    print(\"UMAP fit complete.\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14,10))\n",
    "    unlabeled_mask = all_lbls == -1\n",
    "    ax.scatter(emb_2d[unlabeled_mask,0], emb_2d[unlabeled_mask,1], c='lightgray', s=5, label='Unlabeled')\n",
    "    labeled_mask = ~unlabeled_mask\n",
    "    scatter = ax.scatter(emb_2d[labeled_mask,0], emb_2d[labeled_mask,1], c=all_lbls[labeled_mask], cmap='Spectral', s=20)\n",
    "    \n",
    "    if len(dbss_idx) > 0:\n",
    "        ax.scatter(emb_2d[len(labeled_embs):][dbss_idx], emb_2d[len(labeled_embs):][dbss_idx], c='red', s=100, marker='x', label='DBSS')\n",
    "    if len(ssas_idx) > 0:\n",
    "        ax.scatter(emb_2d[len(labeled_embs):][ssas_idx], emb_2d[len(labeled_embs):][ssas_idx], c='lime', s=100, marker='+', label='SSAS')\n",
    "    \n",
    "    ax.set_title(f'Feature Space UMAP - Cycle {cycle}')\n",
    "    class_legend = ax.legend(handles=scatter.legend_elements()[0], labels=all_class_names, title=\"Classes\")\n",
    "    ax.add_artist(class_legend)\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=[h for h in handles if h.get_label() in ['Unlabeled', 'DBSS', 'SSAS']])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # (FIXED) Removed local_rank error\n",
    "    wandb.log({f\"AL_Cycle_{cycle}/umap\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "def log_confusion_matrix(preds, lbls, names):\n",
    "    cm = confusion_matrix(lbls, preds, labels=range(len(names))) \n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues', xticklabels=names, yticklabels=names)\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True'); ax.set_title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    # (FIXED) Removed local_rank error\n",
    "    wandb.log({\"AL_ConfMatrix\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"AL components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models: Full RFB-ESRGAN Generator (to match saved state_dict), RelDiscriminator, PerceptualLoss\n",
    "# (Based on ESRGAN; assume growth=32 for lightweight)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.hub\n",
    "import math\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, nc=64, growth=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(nc, growth, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nc + growth, growth, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nc + 2*growth, growth, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nc + 3*growth, growth, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nc + 4*growth, nc, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat([x, x1], 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat([x, x1, x2], 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat([x, x1, x2, x3], 1)))\n",
    "        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], 1))\n",
    "        # FIX: Light anti-aliasing blur to reduce checkerboard in residuals\n",
    "        x5 = F.avg_pool2d(x5, kernel_size=3, stride=1, padding=1)\n",
    "        return x5 * 0.2 + x  # Dense residual\n",
    "\n",
    "class RFBESRGANGenerator(nn.Module):\n",
    "    def __init__(self, growth=32, num_blocks=23, nc=64, upscale=4):\n",
    "        super().__init__()\n",
    "        self.entry = nn.Sequential(nn.Conv2d(3, nc, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.body = nn.ModuleList([ResidualDenseBlock(nc, growth) for _ in range(num_blocks)])\n",
    "        self.conv_tail = nn.Conv2d(nc, nc, 3, 1, 1)\n",
    "        # FIX: Bilinear upsample + Conv (replaces TransposedConv to avoid checkerboard artifacts)\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(nc, nc//2, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(nc//2, 3, 3, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry(x)\n",
    "        res = x\n",
    "        for block in self.body:\n",
    "            x = block(x)\n",
    "        x = self.conv_tail(x)\n",
    "        x += res  # Global skip\n",
    "        x = self.up(x)\n",
    "        return torch.tanh(x)  # [-1,1] range\n",
    "\n",
    "class RelDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0)  # PatchGAN output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True).features[:35].eval()\n",
    "        for p in vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        # FIX: Use next(param).device for Sequential (vgg has no direct .device)\n",
    "        vgg_device = next(self.vgg.parameters()).device\n",
    "        sr = sr.to(vgg_device)\n",
    "        hr = hr.to(vgg_device)\n",
    "        sr_vgg = (self.vgg((sr + 1)/2 * self.std + self.mean) + 1)/2\n",
    "        hr_vgg = (self.vgg((hr + 1)/2 * self.std + self.mean) + 1)/2\n",
    "        return self.l1(sr_vgg, hr_vgg)\n",
    "\n",
    "# PSNR/SSIM Helpers (FIX: Smaller win_size for small images; skip zero-var)\n",
    "def psnr(sr, hr, max_val=1.0):\n",
    "    mse = F.mse_loss(sr, hr)\n",
    "    if mse == 0: return 100.0\n",
    "    return 20 * math.log10(max_val / math.sqrt(mse.item()))\n",
    "\n",
    "def compute_ssim(sr, hr):\n",
    "    sr_np = sr.permute(1,2,0).cpu().numpy()\n",
    "    hr_np = hr.permute(1,2,0).cpu().numpy()\n",
    "    # FIX: Skip if zero-variance (causes divide-by-zero in HSV)\n",
    "    if sr_np.std() < 1e-6 or hr_np.std() < 1e-6:\n",
    "        return 1.0  # Perfect if both black\n",
    "    # win_size=3 for 30x30 LR; channel_axis=-1 for HWC RGB\n",
    "    return ssim(sr_np, hr_np, multichannel=True, channel_axis=-1, data_range=1.0, win_size=3)\n",
    "\n",
    "print(\"SR Models defined.\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# SECTION 3 (CONTINUED): CLASSIFIER MODEL DEFINITIONS\n",
    "# (Moved from Cell 5 to solve NameError)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block\"\"\"\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), # Squeeze\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c, c // r, bias=False), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Linear(c // r, c, bias=False), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        return x * y.view(x.shape[0], x.shape[1], 1, 1) # Excitation\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"ResNet-style block with SE\"\"\"\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.se = SEBlock(out_c)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride > 1 or in_c != out_c:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out) # Apply attention\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class RobustClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    The main classifier model.\n",
    "    Takes 64x64 SR images as input.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_blocks=[3,4,6,3]):\n",
    "        super().__init__()\n",
    "        self.in_c = 64\n",
    "        # Initial 7x7 conv to reduce dimensions\n",
    "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False); self.bn1 = nn.BatchNorm2d(64) # 64x64 -> 32x32\n",
    "        # Stack layers\n",
    "        self.layer1 = self._make_layer(64, num_blocks[0], stride=1) # 32x32\n",
    "        self.layer2 = self._make_layer(128, num_blocks[1], stride=2) # 16x16\n",
    "        self.layer3 = self._make_layer(256, num_blocks[2], stride=2) # 8x8\n",
    "        self.layer4 = self._make_layer(512, num_blocks[3], stride=2) # 4x4\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, out_c, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(BasicBlock(self.in_c, out_c, s))\n",
    "            self.in_c = out_c\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
    "        x = self.pool(x)\n",
    "        return self.fc(x.flatten(1))\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
    "        return self.pool(x).flatten(1)\n",
    "\n",
    "print(\"Classifier Models defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-26T11:52:16.857707Z",
     "iopub.status.idle": "2025-10-26T11:52:16.857927Z",
     "shell.execute_reply": "2025-10-26T11:52:16.857825Z",
     "shell.execute_reply.started": "2025-10-26T11:52:16.857815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# SECTION 6: ACTIVE LEARNING PIPELINE\n",
    "# (FIXED: This cell now assumes all models and helpers are defined\n",
    "# in previous cells and uses the CORRECT, real labels from Cell 2)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# We assume 'g' (the SR model) is already trained and exists in memory from Cell 4\n",
    "# We load the *final* saved model for consistency and to ensure it's in eval mode.\n",
    "SR_MODEL_PATH = '/kaggle/working/sr_model_final.pth' # <-- This must match the save path from Cell 4\n",
    "if not os.path.exists(SR_MODEL_PATH):\n",
    "    print(f\"WARNING: {SR_MODEL_PATH} not found! Using model from memory. Run Cell 4 first.\")\n",
    "else:\n",
    "    try:\n",
    "        print(f\"Loading trained SR model from {SR_MODEL_PATH}...\")\n",
    "        # Re-instantiate the model to load the state dict\n",
    "        g = RFBESRGANGenerator(growth=32).to(device)\n",
    "        if gpu_count > 1:\n",
    "            g = nn.DataParallel(g)\n",
    "            g.module.load_state_dict(torch.load(SR_MODEL_PATH, map_location=device))\n",
    "        else:\n",
    "            g.load_state_dict(torch.load(SR_MODEL_PATH, map_location=device))\n",
    "        print(\"Trained SR model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SR model, using model from memory: {e}\")\n",
    "g.eval() # Set to evaluation mode\n",
    "\n",
    "# Instantiate Classifier (from Cell 3)\n",
    "# (FIXED) Pass the correct num_classes and num_blocks\n",
    "clf = RobustClassifier(num_classes=config['num_classes'], num_blocks=[3,4,6,3]).to(device)\n",
    "if gpu_count > 1:\n",
    "    clf = nn.DataParallel(clf)\n",
    "    print(f\"Classifier using DataParallel across {gpu_count} GPUs\")\n",
    "wandb.watch(clf, log=\"all\", log_freq=100)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# (CRITICAL FIX) AL Setup: Use REAL labels for stratification\n",
    "# We assume 'train_idx', 'val_idx', 'al_base_dataset', 'val_loader_al_dataset',\n",
    "# and 'all_class_names' exist from Cell 2.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Get the labels corresponding to our training indices\n",
    "# train_idx is a list of *master indices* from our valid, limited dataset\n",
    "# We need to get the labels for *just* these indices\n",
    "train_labels_subset = [patch_to_label.get(all_b04_paths[i].parent.name) for i in train_idx]\n",
    "# Filter out any None labels just in case\n",
    "valid_train_indices_for_al = [idx for i, idx in enumerate(train_idx) if train_labels_subset[i] is not None and train_labels_subset[i] != -1]\n",
    "valid_train_labels_for_al = [lbl for lbl in train_labels_subset if lbl is not None and lbl != -1]\n",
    "\n",
    "initial_pool_size = int(0.1 * len(valid_train_indices_for_al))\n",
    "\n",
    "# (FIXED) Create the split using the *actual* labels\n",
    "labeled_indices, unlabeled_indices = train_test_split(\n",
    "    valid_train_indices_for_al, \n",
    "    train_size=initial_pool_size, \n",
    "    random_state=42,\n",
    "    stratify=valid_train_labels_for_al # <-- This is the critical fix\n",
    ")\n",
    "print(f\"AL Setup: Labeled pool {len(labeled_indices)}, Unlabeled pool {len(unlabeled_indices)}\")\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# AL Dataset for subsets\n",
    "class IndexedALDataset(Dataset):\n",
    "    \"\"\"Wraps the BigEarthNetSR dataset to use a specific list of indices.\"\"\"\n",
    "    def __init__(self, base_ds, indices):\n",
    "        self.base_ds = base_ds\n",
    "        self.indices = indices # These are the *master* indices\n",
    "    \n",
    "    def __len__(self): return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # self.indices[idx] gives the *original* master index\n",
    "        original_idx = self.indices[idx]\n",
    "        # Get the item from the *original* list of all_b04_paths\n",
    "        # The base_ds __getitem__ needs the index relative to *its* internal list\n",
    "        # Let's fix the base_ds to accept master indices directly\n",
    "        \n",
    "        # We need to find the item in al_base_dataset that corresponds to original_idx\n",
    "        # This is complex. Let's redefine IndexedALDataset to use the *original* dataset object\n",
    "        item = self.base_ds[original_idx] \n",
    "        return item\n",
    "\n",
    "# We assume 'al_base_dataset' and 'val_loader_al_dataset' were created in Cell 2\n",
    "# Let's create them here to be 100% sure they are correct\n",
    "al_base_dataset = BigEarthNetSR(image_root_path, list(range(len(all_b04_paths))), all_b04_paths, patch_to_label, scale=4, phase='al')\n",
    "val_al_loader_dataset = BigEarthNetSR(image_root_path, val_idx, all_b04_paths, patch_to_label, scale=4, phase='al')\n",
    "\n",
    "\n",
    "labeled_loader = DataLoader(IndexedALDataset(al_base_dataset, labeled_indices), batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "unlabeled_loader = DataLoader(IndexedALDataset(al_base_dataset, unlabeled_indices), batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "val_al_loader = DataLoader(val_al_loader_dataset, batch_size=16, shuffle=False, num_workers=2) # Use the correct validation set\n",
    "\n",
    "\n",
    "teacher = None\n",
    "for cycle in range(config['al_cycles']):\n",
    "    print(f\"\\n--- AL Cycle {cycle+1}/{config['al_cycles']} ---\")\n",
    "    wandb.log({\"AL_Cycle\": cycle})\n",
    "    \n",
    "    # Train Classifier (using functions from Cell 5)\n",
    "    clf = train_classifier(clf, labeled_loader, config['al_epochs'])\n",
    "    \n",
    "    # Get Embeddings (using functions from Cell 5)\n",
    "    print(\"Extracting embeddings for Labeled pool...\")\n",
    "    labeled_embs, labeled_lbls = get_embeddings(clf, labeled_loader, g)\n",
    "    print(\"Extracting embeddings for Unlabeled pool...\")\n",
    "    if not unlabeled_indices:\n",
    "        print(\"No more unlabeled data to select from.\")\n",
    "        break\n",
    "    unlabeled_embs, _ = get_embeddings(clf, unlabeled_loader, g)\n",
    "    \n",
    "    # DBSS Selection (using functions from Cell 5)\n",
    "    n_select = min(int(len(train_idx) * 0.1), len(unlabeled_indices))\n",
    "    dbss_local_idx = dbss_select(unlabeled_embs, labeled_embs, labeled_lbls, n_select)\n",
    "    \n",
    "    # Map local indices (0 to len(unlabeled_embs)) back to original dataset indices\n",
    "    newly_labeled_human_indices = [unlabeled_indices[i] for i in dbss_local_idx]\n",
    "    \n",
    "    # SSAS (post-cycle 1)\n",
    "    newly_labeled_pseudo_indices = []\n",
    "    ssas_local_viz_idx = np.array([])\n",
    "    if cycle >= 1 and teacher is not None:\n",
    "        print(\"Running SSAS...\")\n",
    "        pseudo_original_indices = ssas_pseudo(clf, teacher, g, unlabeled_loader)\n",
    "        newly_labeled_pseudo_indices = [idx for idx in pseudo_original_indices if idx not in newly_labeled_human_indices]\n",
    "        \n",
    "        # Get local indices for SSAS for visualization\n",
    "        unlabeled_map = {original_idx: local_idx for local_idx, original_idx in enumerate(unlabeled_indices)}\n",
    "        ssas_local_viz_idx = [unlabeled_map.get(pi) for pi in newly_labeled_pseudo_indices if pi in unlabeled_map]\n",
    "        ssas_local_viz_idx = np.array([idx for idx in ssas_local_viz_idx if idx is not None and idx < len(unlabeled_embs)])\n",
    "\n",
    "    \n",
    "    # Update Pools\n",
    "    all_new_indices = np.unique(np.concatenate([newly_labeled_human_indices, newly_labeled_pseudo_indices])).tolist()\n",
    "    labeled_indices = np.unique(np.concatenate([labeled_indices, all_new_indices])).tolist()\n",
    "    unlabeled_indices = np.setdiff1d(unlabeled_indices, all_new_indices).tolist()\n",
    "    \n",
    "    print(f\"Cycle {cycle+1} complete. Added {len(newly_labeled_human_indices)} DBSS samples and {len(newly_labeled_pseudo_indices)} SSAS samples.\")\n",
    "    print(f\"New pool sizes: Labeled={len(labeled_indices)}, Unlabeled={len(unlabeled_indices)}\")\n",
    "\n",
    "    # Log Visuals (UMAP)\n",
    "    log_umap(labeled_embs, labeled_lbls, unlabeled_embs, dbss_local_idx, ssas_local_viz_idx, cycle)\n",
    "    \n",
    "    # Update Teacher (deepcopy handles DataParallel)\n",
    "    teacher = copy.deepcopy(clf)\n",
    "    \n",
    "    # Cycle Eval (FIXED: Using correct single-label evaluation)\n",
    "    val_acc, val_preds, val_lbls = evaluate_model(clf, val_al_loader, g) \n",
    "    wandb.log({f\"AL_Cycle_{cycle}/accuracy\": val_acc})\n",
    "    log_confusion_matrix(val_preds, val_lbls, all_class_names) # (FIXED) Use correct class_names\n",
    "    print(f\"Cycle {cycle} Val Acc (Standard): {val_acc:.2f}%\")\n",
    "    \n",
    "    if not unlabeled_indices:\n",
    "        print(\"All samples labeled. Halting.\")\n",
    "        break\n",
    "        \n",
    "    # Update Loaders\n",
    "    labeled_loader = DataLoader(IndexedALDataset(al_base_dataset, labeled_indices), batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "    unlabeled_loader = DataLoader(IndexedALDataset(al_base_dataset, unlabeled_indices), batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "# Save Classifier (unwrap if DataParallel)\n",
    "def save_clf_model(model, path): # (FIXED) Define save_model locally in cell\n",
    "    state_dict = model.module.state_dict() if gpu_count > 1 else model.state_dict()\n",
    "    torch.save(state_dict, path)\n",
    "    wandb.save(path, policy=\"now\")\n",
    "    print(f\"Saved {path}\")\n",
    "    \n",
    "save_clf_model(clf, '/kaggle/working/clf_model.pth')\n",
    "\n",
    "print(\"AL Pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cell 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-26T11:52:16.858641Z",
     "iopub.status.idle": "2025-10-26T11:52:16.858914Z",
     "shell.execute_reply": "2025-10-26T11:52:16.858813Z",
     "shell.execute_reply.started": "2025-10-26T11:52:16.858798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# SECTION 7: FINAL EVALUATION\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Starting final evaluation...\")\n",
    "\n",
    "# We assume 'clf', 'val_loader', and 'g' (SR model from input path) exist from Cell 6\n",
    "# We assume 'psnr' and 'compute_ssim' exist from Cell 3\n",
    "\n",
    "final_acc, final_preds, final_lbls = evaluate_model(clf, val_loader, g)\n",
    "\n",
    "# Calculate final PSNR/SSIM on a subset of the validation loader\n",
    "final_psnr, final_ssim, batch_count = 0, 0, 0\n",
    "val_iter = iter(val_loader)\n",
    "for _ in range(10): # Use 10 batches for a good approximation\n",
    "    try:\n",
    "        batch = next(val_iter)\n",
    "        lr_v, hr_v = batch['lr'].to(device), batch['hr'].to(device)\n",
    "        with torch.no_grad():\n",
    "            sr_v = g(lr_v) if gpu_count == 1 else g.module(lr_v)\n",
    "        final_psnr += psnr(sr_v, hr_v)  # Batch PSNR\n",
    "        # SSIM: Loop over batch (per-image)\n",
    "        batch_ssim = 0\n",
    "        for b in range(lr_v.shape[0]):\n",
    "            batch_ssim += compute_ssim(sr_v[b], hr_v[b])\n",
    "        final_ssim += batch_ssim / lr_v.shape[0]\n",
    "        batch_count += 1\n",
    "    except StopIteration:\n",
    "        break # Stop if val_loader has fewer than 10 batches\n",
    "    except Exception as e:\n",
    "        print(f\"Error in final eval: {e}\")\n",
    "\n",
    "if batch_count > 0:\n",
    "    final_psnr /= batch_count\n",
    "    final_ssim /= batch_count\n",
    "\n",
    "wandb.log({\n",
    "    \"final/psnr\": final_psnr, \"final/ssim\": final_ssim,\n",
    "    \"final/accuracy\": final_acc  # Hamming\n",
    "})\n",
    "\n",
    "# Summary Table\n",
    "table = wandb.Table(columns=[\"Metric\", \"Value\"])\n",
    "table.add_data([\"PSNR (dB)\", f\"{final_psnr:.2f}\"])\n",
    "table.add_data([\"SSIM\", f\"{final_ssim:.3f}\"])\n",
    "table.add_data([\"AL Accuracy (Hamming %)\", f\"{final_acc:.2f}\"])\n",
    "wandb.log({\"final/summary\": table})\n",
    "\n",
    "print(f\"Final PSNR: {final_psnr:.2f} | SSIM: {final_ssim:.3f} | AL Acc: {final_acc:.2f}%\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Pipeline complete! Check WandB dashboard for full logs/visuals.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 705367,
     "sourceId": 1231934,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5872309,
     "sourceId": 9621334,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 479817,
     "modelInstanceId": 464024,
     "sourceId": 617219,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
